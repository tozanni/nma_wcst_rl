{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tozanni/nma_wcst_rl/blob/main/human_rl_wcst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BHbUO_QW2nOV"
      },
      "source": [
        "# Using RL to Model Wisconsin Card Sorting Task\n",
        "\n",
        "\n",
        "**Original notebook credits:**\n",
        "\n",
        "__Content creators:__ Morteza Ansarinia, Yamil Vidal\n",
        "\n",
        "__Production editor:__ Spiros Chavlis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kBCTQKcY2nOg"
      },
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "- This project aims to use behavioral data to train an agent and then use the agent to investigate data produced by human subjects. Having a computational agent that mimics humans in such tests, we will be able to compare its mechanics with human data.\n",
        "\n",
        "- In another conception, we could fit an agent that learns many cognitive tasks that require abstract-level constructs such as executive functions. This is a multi-task control problem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ApJnFBK72nOi"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "yHw6zo6R2nOj",
        "outputId": "9458ae3c-946e-4d59-bdd5-d15ef5a04ec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.5/318.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for dm-acme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.4/268.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.82 requires numpy>=1.25.0, but you have numpy 1.23.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!pip install jedi --quiet\n",
        "!pip install --upgrade pip setuptools wheel --quiet\n",
        "!pip install dm-acme[jax] --quiet\n",
        "!pip install dm-sonnet --quiet\n",
        "!pip install trfl --quiet\n",
        "!pip install numpy==1.23.3 --quiet --ignore-installed\n",
        "!pip uninstall seaborn -y --quiet\n",
        "!pip install seaborn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {},
        "id": "8q8RNDSn2nOq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "3add327a-be60-4d16-e614-7ccbc7a3e483"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/reverb/platform/default/ensure_tf_install.py:53: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(version) <\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sonnet as snt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import dm_env\n",
        "\n",
        "import acme\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme import EnvironmentLoop\n",
        "from acme.agents.tf import dqn\n",
        "from acme.utils import loggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "5j0ZUwlV2nOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d47bbfc-69d7-41b8-a9b6-fb8030dc9fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# @title Figure settings\n",
        "from IPython.display import clear_output, display, HTML\n",
        "%matplotlib inline\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "yeSwZpB_2nOt"
      },
      "source": [
        "---\n",
        "# Background\n",
        "\n",
        "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Jh-sqwiF2nOw"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "This notebook works on simulated data only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "liKW-N012nOx"
      },
      "source": [
        "## N-back task\n",
        "\n",
        "In the N-back task, participants view a sequence of stimuli, one per time, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedbacks are given at both timestep and trajectory levels.\n",
        "\n",
        "In a typical neuro setup, both accuracy and response time are measured, but here, for the sake of brevity, we focus only on accuracy of responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EdljINN32nOy"
      },
      "source": [
        "---\n",
        "# Cognitive Tests Environment\n",
        "\n",
        "First we develop an environment in that agents perform a cognitive test, here the N-back."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "vvv7SFOc2nO1"
      },
      "source": [
        "## Implementation scheme\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "AgKPh5QN2nO2"
      },
      "source": [
        "### Environment\n",
        "\n",
        "The following cell implments N-back envinronment, that we later use to train a RL agent on human data. It is capable of performing two kinds of simulation:\n",
        "- rewards the agent once the action was correct (i.e., a normative model of the environment).\n",
        "- receives human data (or mock data if you prefer), and returns what participants performed as the observation. This is more useful for preference-based RL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {},
        "id": "__V0gMzt2nO2"
      },
      "outputs": [],
      "source": [
        "class NBack(dm_env.Environment):\n",
        "\n",
        "  ACTIONS = ['match', 'non-match']\n",
        "\n",
        "  def __init__(self,\n",
        "               N=2,\n",
        "               episode_steps=32,\n",
        "               stimuli_choices=list('ABCDEF'),\n",
        "               human_data=None,\n",
        "               seed=1,\n",
        "               ):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      N: Number of steps to look back for the matched stimuli. Defaults to 2 (as in 2-back).\n",
        "      episode_steps\n",
        "      stimuli_choices\n",
        "      human_data\n",
        "      seed\n",
        "\n",
        "    \"\"\"\n",
        "    self.N = N\n",
        "    self.episode_steps = episode_steps\n",
        "    self.stimuli_choices = stimuli_choices\n",
        "    self.stimuli = np.empty(shape=episode_steps)  # will be filled in the `reset()`\n",
        "    self._reset_next_step = True\n",
        "    self._action_history = []\n",
        "\n",
        "  def reset(self):\n",
        "    self._reset_next_step = False\n",
        "    self._current_step = 0\n",
        "    self._action_history.clear()\n",
        "\n",
        "    # generate a random sequence instead of relying on human data\n",
        "    # self.stimuli = np.random.choice(self.stimuli_choices, self.episode_steps)\n",
        "    # FIXME This is a fix for acme & reverb issue with string observation. Agent should be able to handle strings\n",
        "    self.stimuli = np.random.choice(len(self.stimuli_choices), self.episode_steps).astype(np.float32)\n",
        "\n",
        "    return dm_env.restart(self._observation())\n",
        "\n",
        "\n",
        "  def _episode_return(self):\n",
        "      return 0.0\n",
        "\n",
        "  def step(self, action: int):\n",
        "    if self._reset_next_step:\n",
        "      return self.reset()\n",
        "\n",
        "    agent_action = NBack.ACTIONS[action]\n",
        "\n",
        "    # assume the agent is rationale and doesn't want to reproduce human, reward once the response it correct\n",
        "    expected_action = 'match' if (self.stimuli[self._current_step] == self.stimuli[self._current_step - self.N]) else 'non-match'\n",
        "    step_reward = 0. if (agent_action == expected_action) else -1.\n",
        "    self._action_history.append(agent_action)\n",
        "    self._current_step += 1\n",
        "\n",
        "    # Check for termination.\n",
        "    if self._current_step == self.stimuli.shape[0]:\n",
        "      self._reset_next_step = True\n",
        "      # we are using the mean of total time step rewards as the episode return\n",
        "      return dm_env.termination(reward=self._episode_return(),\n",
        "                                observation=self._observation())\n",
        "    else:\n",
        "      return dm_env.transition(reward=step_reward,\n",
        "                               observation=self._observation())\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return dm_env.specs.BoundedArray(\n",
        "        shape=self.stimuli.shape,\n",
        "        dtype=self.stimuli.dtype,\n",
        "        name='nback_stimuli', minimum=0, maximum=len(self.stimuli_choices) + 1)\n",
        "\n",
        "  def action_spec(self):\n",
        "    return dm_env.specs.DiscreteArray(\n",
        "        num_values=len(NBack.ACTIONS),\n",
        "        dtype=np.int32,\n",
        "        name='action')\n",
        "\n",
        "  def _observation(self):\n",
        "    # agent observes only the current trial\n",
        "    # obs = self.stimuli[self._current_step - 1]\n",
        "    # agents observe stimuli up to the current trial\n",
        "    obs = self.stimuli[:self._current_step+1].copy()\n",
        "    obs = np.pad(obs,(0, len(self.stimuli) - len(obs)))\n",
        "\n",
        "    return obs\n",
        "\n",
        "  def plot_state(self):\n",
        "    \"\"\"Display current state of the environment.\n",
        "\n",
        "     Note: `M` mean `match`, and `.` is a `non-match`.\n",
        "    \"\"\"\n",
        "    stimuli = self.stimuli[:self._current_step - 1]\n",
        "    actions = ['M' if a=='match' else '.' for a in self._action_history[:self._current_step - 1]]\n",
        "    return HTML(\n",
        "        f'<b>Environment ({self.N}-back):</b><br />'\n",
        "        f'<pre><b>Stimuli:</b> {\"\".join(map(str,map(int,stimuli)))}</pre>'\n",
        "        f'<pre><b>Actions:</b> {\"\".join(actions)}</pre>'\n",
        "    )\n",
        "\n",
        "  @staticmethod\n",
        "  def create_environment():\n",
        "    \"\"\"Utility function to create a N-back environment and its spec.\"\"\"\n",
        "\n",
        "    # Make sure the environment outputs single-precision floats.\n",
        "    environment = wrappers.SinglePrecisionWrapper(NBack())\n",
        "\n",
        "    # Grab the spec of the environment.\n",
        "    environment_spec = specs.make_environment_spec(environment)\n",
        "\n",
        "    return environment, environment_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rDXuNYEN2nO4"
      },
      "source": [
        "### Define a random agent\n",
        "\n",
        "For more information you can refer to NMA-DL W3D2 Basic Reinforcement learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {},
        "id": "PTGxhONI2nO4"
      },
      "outputs": [],
      "source": [
        "class RandomAgent(acme.Actor):\n",
        "\n",
        "  def __init__(self, environment_spec):\n",
        "    \"\"\"Gets the number of available actions from the environment spec.\"\"\"\n",
        "    self._num_actions = environment_spec.actions.num_values\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    \"\"\"Selects an action uniformly at random.\"\"\"\n",
        "    action = np.random.randint(self._num_actions)\n",
        "    return action\n",
        "\n",
        "  def observe_first(self, timestep):\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def update(self):\n",
        "    \"\"\"Does not update as the RandomAgent does not learn from data.\"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "MGhCqTtE2nO5"
      },
      "source": [
        "### Initialize the environment and the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {},
        "id": "_XSWBTOd2nO5",
        "outputId": "1f3be93e-7510-41c3-83d0-5c8c31239428",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actions:\n",
            " DiscreteArray(shape=(), dtype=int32, name=action, minimum=0, maximum=1, num_values=2)\n",
            "observations:\n",
            " BoundedArray(shape=(32,), dtype=dtype('float32'), name='nback_stimuli', minimum=0.0, maximum=7.0)\n",
            "rewards:\n",
            " Array(shape=(), dtype=dtype('float32'), name='reward')\n"
          ]
        }
      ],
      "source": [
        "env, env_spec = NBack.create_environment()\n",
        "agent = RandomAgent(env_spec)\n",
        "\n",
        "print('actions:\\n', env_spec.actions)\n",
        "print('observations:\\n', env_spec.observations)\n",
        "print('rewards:\\n', env_spec.rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "mTiFurZ_2nO6"
      },
      "source": [
        "### Run the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {},
        "id": "xjCQ0AhA2nO6"
      },
      "outputs": [],
      "source": [
        "# fitting parameters\n",
        "n_episodes = 100\n",
        "n_total_steps = 0\n",
        "log_loss = False\n",
        "n_steps = n_episodes * 32\n",
        "all_returns = []\n",
        "\n",
        "# main loop\n",
        "for episode in range(n_episodes):\n",
        "  episode_steps = 0\n",
        "  episode_return = 0\n",
        "  episode_loss = 0\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  timestep = env.reset()\n",
        "\n",
        "  # Make the first observation.\n",
        "  agent.observe_first(timestep)\n",
        "\n",
        "  # Run an episode\n",
        "  while not timestep.last():\n",
        "\n",
        "    # DEBUG\n",
        "    # print(timestep)\n",
        "\n",
        "    # Generate an action from the agent's policy and step the environment.\n",
        "    action = agent.select_action(timestep.observation)\n",
        "    timestep = env.step(action)\n",
        "\n",
        "    # Have the agent observe the timestep and let the agent update itself.\n",
        "    agent.observe(action, next_timestep=timestep)\n",
        "    agent.update()\n",
        "\n",
        "    # Book-keeping.\n",
        "    episode_steps += 1\n",
        "    n_total_steps += 1\n",
        "    episode_return += timestep.reward\n",
        "\n",
        "    if log_loss:\n",
        "      episode_loss += agent.last_loss\n",
        "\n",
        "    if n_steps is not None and n_total_steps >= n_steps:\n",
        "      break\n",
        "\n",
        "  # Collect the results and combine with counts.\n",
        "  steps_per_second = episode_steps / (time.time() - start_time)\n",
        "  result = {\n",
        "      'episode': episode,\n",
        "      'episode_length': episode_steps,\n",
        "      'episode_return': episode_return,\n",
        "  }\n",
        "  if log_loss:\n",
        "    result['loss_avg'] = episode_loss/episode_steps\n",
        "\n",
        "  all_returns.append(episode_return)\n",
        "\n",
        "  display(env.plot_state())\n",
        "  # Log the given results.\n",
        "  print(result)\n",
        "\n",
        "  if n_steps is not None and n_total_steps >= n_steps:\n",
        "    break\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Histogram of all returns\n",
        "#plt.figure()\n",
        "#sns.histplot(all_returns, stat=\"density\", kde=True, bins=12)\n",
        "#plt.xlabel('Return [a.u.]')\n",
        "#plt.ylabel('Density')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xgUDEuym2nO7"
      },
      "source": [
        "**Note:** You can simplify the environment loop using [DeepMind Acme](https://github.com/deepmind/acme)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {},
        "id": "l6wSUwW82nO8"
      },
      "outputs": [],
      "source": [
        "# init a new N-back environment\n",
        "env, env_spec = NBack.create_environment()\n",
        "\n",
        "# DEBUG fake testing environment.\n",
        "# Uncomment this to debug your agent without using the N-back environment.\n",
        "# env = fakes.DiscreteEnvironment(\n",
        "#     num_actions=2,\n",
        "#     num_observations=1000,\n",
        "#     obs_dtype=np.float32,\n",
        "#     episode_length=32)\n",
        "# env_spec = specs.make_environment_spec(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {},
        "id": "nFpI1uaN2nO9"
      },
      "outputs": [],
      "source": [
        "def dqn_make_network(action_spec: specs.DiscreteArray) -> snt.Module:\n",
        "  return snt.Sequential([\n",
        "      snt.Flatten(),\n",
        "      snt.nets.MLP([50, 50, action_spec.num_values]),\n",
        "  ])\n",
        "\n",
        "# construct a DQN agent\n",
        "agent = dqn.DQN(\n",
        "    environment_spec=env_spec,\n",
        "    network=dqn_make_network(env_spec.actions),\n",
        "    epsilon=[0.5],\n",
        "    logger=loggers.InMemoryLogger(),\n",
        "    checkpoint=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JfcFCx1L2nO9"
      },
      "source": [
        "Now, we run the environment loop with the DQN agent and print the training log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {},
        "id": "ZoG8b0FF2nO-"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "loop = EnvironmentLoop(env, agent, logger=loggers.InMemoryLogger())\n",
        "#loop.run(n_episodes)\n",
        "\n",
        "# print logs\n",
        "#logs = pd.DataFrame(loop._logger._data)\n",
        "#logs.tail()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}