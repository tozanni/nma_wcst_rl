{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tozanni/nma_wcst_rl/blob/main/human_rl_wcst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BHbUO_QW2nOV"
      },
      "source": [
        "# Using RL to Model Wisconsin Card Sorting Task\n",
        "\n",
        "\n",
        "**Original notebook credits:**\n",
        "\n",
        "__Content creators:__ Morteza Ansarinia, Yamil Vidal\n",
        "\n",
        "__Production editor:__ Spiros Chavlis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kBCTQKcY2nOg"
      },
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "- This project aims to use behavioral data to train an agent and then use the agent to investigate data produced by human subjects. Having a computational agent that mimics humans in such tests, we will be able to compare its mechanics with human data.\n",
        "\n",
        "- In another conception, we could fit an agent that learns many cognitive tasks that require abstract-level constructs such as executive functions. This is a multi-task control problem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ApJnFBK72nOi"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "yHw6zo6R2nOj",
        "outputId": "2100bcd7-ce34-4931-8cf0-e495c81fdfdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.5/318.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for dm-acme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.4/268.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.82 requires numpy>=1.25.0, but you have numpy 1.23.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!pip install jedi --quiet\n",
        "!pip install --upgrade pip setuptools wheel --quiet\n",
        "!pip install dm-acme[jax] --quiet\n",
        "!pip install dm-sonnet --quiet\n",
        "!pip install trfl --quiet\n",
        "!pip install numpy==1.23.3 --quiet --ignore-installed\n",
        "!pip uninstall seaborn -y --quiet\n",
        "!pip install seaborn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {},
        "id": "8q8RNDSn2nOq",
        "outputId": "baed276b-fb21-493b-875c-8c59ac828982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/reverb/platform/default/ensure_tf_install.py:53: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(version) <\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sonnet as snt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import dm_env\n",
        "\n",
        "import acme\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme import EnvironmentLoop\n",
        "from acme.agents.tf import dqn\n",
        "from acme.utils import loggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "5j0ZUwlV2nOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df172ff7-1f82-4f93-f660-2414fdee3e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# @title Figure settings\n",
        "from IPython.display import clear_output, display, HTML\n",
        "%matplotlib inline\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "yeSwZpB_2nOt"
      },
      "source": [
        "---\n",
        "# Background\n",
        "\n",
        "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Jh-sqwiF2nOw"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "This notebook works on simulated data only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "liKW-N012nOx"
      },
      "source": [
        "## Wisconsin Card Sorting task (WCST)\n",
        "\n",
        "TODO: Describe Task\n",
        "\n",
        "TODO: Describe metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EdljINN32nOy"
      },
      "source": [
        "---\n",
        "# Cognitive Tests Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "vvv7SFOc2nO1"
      },
      "source": [
        "## Implementation scheme\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a random agent\n",
        "\n",
        "For more information you can refer to NMA-DL W3D2 Basic Reinforcement learning.\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ar84zoeL3cy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAgent(acme.Actor):\n",
        "\n",
        "  def __init__(self, environment_spec):\n",
        "    \"\"\"Gets the number of available actions from the environment spec.\"\"\"\n",
        "    self._num_actions = environment_spec.actions.num_values\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    \"\"\"Selects an action uniformly at random.\"\"\"\n",
        "    action = np.random.randint(self._num_actions)\n",
        "    return action\n",
        "\n",
        "  def observe_first(self, timestep):\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def update(self):\n",
        "    \"\"\"Does not update as the RandomAgent does not learn from data.\"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "Tz83qGcBx9ju"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WCST utility methods\n"
      ],
      "metadata": {
        "id": "3W-5wv-y3hfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module WCST (Import as regular functions)\n",
        "\n",
        "Generates cards for the different WCST tasks.\n",
        "\n",
        "Pauline Bock\n",
        "09-04-2019\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "def perception(nb_dim, nb_templates, nb_features):\n",
        "    \"\"\"\n",
        "    Create and return reference cards.\n",
        "    \"\"\"\n",
        "    percep_shape = (nb_templates, nb_dim, nb_features)\n",
        "    percep = np.zeros(percep_shape, dtype=int)\n",
        "\n",
        "    #Random generating of binary features\n",
        "    for i in range(0, nb_templates):\n",
        "        for j in range(0, nb_dim):\n",
        "            percep[i][j].put([i], 1)\n",
        "\n",
        "    return percep\n",
        "\n",
        "def response_item_Reasoning(nb_dim, nb_features, m_percep, reasoning_list):\n",
        "    \"\"\"\n",
        "    Create and return a card for Reasoning version that was not already created. (36 ambiguous cards).\n",
        "    \"\"\"\n",
        "    item = create_card_Reasoning(nb_dim, nb_features)\n",
        "    #check if different from reference cards\n",
        "    eq = check_equality(m_percep, item, nb_dim)\n",
        "    unique = check_unity(item, reasoning_list)\n",
        "\n",
        "    while(eq == 1 or unique==1):\n",
        "        item = create_card_Reasoning(nb_dim, nb_features)\n",
        "        eq = check_equality(m_percep, item, nb_dim)\n",
        "        unique = check_unity(item, reasoning_list)\n",
        "\n",
        "    reasoning_list.append(item)\n",
        "    return item\n",
        "\n",
        "def check_unity(item, cardlist):\n",
        "    if inList(item, cardlist) == True:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def create_card_Reasoning(nb_dim, nb_features):\n",
        "    \"\"\"\n",
        "    Create a card with ambiguity.\n",
        "    \"\"\"\n",
        "    item_shape = (nb_dim, nb_features)\n",
        "    item = np.zeros(item_shape, dtype=int)\n",
        "\n",
        "    r = [0,1,2]\n",
        "    #random 2 different dimensions\n",
        "    idim1 = random.randint(0,2)\n",
        "    dim1 = r[idim1]\n",
        "    r.remove(dim1)\n",
        "    idim2 = random.randint(0,len(r)-1)\n",
        "    dim2 = r[idim2]\n",
        "    r.remove(dim2)\n",
        "    dim3 = r[0]\n",
        "\n",
        "    feat = random.randint(0,3)\n",
        "    feat2 = random.randint(0,3)\n",
        "    while(feat == feat2):\n",
        "        feat2 = random.randint(0,3)\n",
        "\n",
        "    np.put(item[dim1],[feat], 1)\n",
        "    np.put(item[dim2],[feat], 1)\n",
        "    np.put(item[dim3],[feat2], 1)\n",
        "\n",
        "    return item\n",
        "\n",
        "\n",
        "def check_equality(m_percep, item, nb_dim):\n",
        "    \"\"\"\n",
        "    Check if the card is different from the reference ones.\n",
        "    \"\"\"\n",
        "    for temp in range(0, m_percep.shape[0]):\n",
        "        dim_eq = 0\n",
        "        for dim in range(0, nb_dim):\n",
        "            if np.array_equal(m_percep[temp][dim], item[dim]):\n",
        "                dim_eq +=1\n",
        "\n",
        "        if dim_eq == nb_dim:\n",
        "            return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "def inList(array, arraylist):\n",
        "    for element in arraylist:\n",
        "        if np.array_equal(element, array):\n",
        "            return True\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "-UCK5IPrgRaY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "AgKPh5QN2nO2"
      },
      "source": [
        "### Environment\n",
        "\n",
        "The following cell implments an envinronment for the WCST:\n",
        "- Rewards the agent once the action was correct (i.e., a normative model of the environment).\n",
        "- **Future work**: Receives human data and returns what participants performed as the observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "execution": {},
        "id": "__V0gMzt2nO2"
      },
      "outputs": [],
      "source": [
        "class WCST_Env(dm_env.Environment):\n",
        "    ACTIONS = [0, 1, 2, 3]\n",
        "\n",
        "    def __init__(self,seed=1):\n",
        "\n",
        "        self.episode_steps = 36  #36 cards or steps per episode\n",
        "        self._current_step = 0  #Current episode step counter\n",
        "        self._reset_next_step = True\n",
        "        self._action_history = []\n",
        "\n",
        "        #Init WCST variables\n",
        "        self.nb_dim = 3\n",
        "        self.nb_features = 4\n",
        "        self.nb_templates = self.nb_features\n",
        "        r = 3  #rules number, we have 3 rules\n",
        "\n",
        "        self.sample_card = np.array([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1])\n",
        "\n",
        "        self.nbTS = 0\n",
        "        self.nb_win = 0\n",
        "        self.t_criterion = 0\n",
        "        self.t_err = 0\n",
        "        self.criterions = []\n",
        "\n",
        "        self.winstreak = 0\n",
        "        self.m_percep = perception(self.nb_dim, self.nb_templates, self.nb_features)\n",
        "        self.seen_cards = []\n",
        "        self.rule = 0\n",
        "\n",
        "        #Last card info, it's dealt for the first time on reset method\n",
        "        self.np_data = []\n",
        "        self.v_data = []\n",
        "\n",
        "    def new_card(self):\n",
        "        v_data = [] #list type\n",
        "        np_data = response_item_Reasoning(self.nb_dim, self.nb_features, self.m_percep, self.seen_cards) #Modified WCST version\n",
        "\n",
        "        #Transform into a vector\n",
        "        for arr in np_data:\n",
        "            for e in arr:\n",
        "                v_data.append(e)\n",
        "\n",
        "        #Save last card info\n",
        "        self.np_data = np_data\n",
        "        self.v_data = v_data\n",
        "\n",
        "        return np_data, v_data\n",
        "\n",
        "    def reset(self):\n",
        "        self._reset_next_step = False\n",
        "        self._current_step = 0\n",
        "        self.seen_cards = []\n",
        "        self._action_history.clear()\n",
        "\n",
        "        #Deal new card\n",
        "        return dm_env.restart(self._observation())\n",
        "\n",
        "    def _episode_return(self):\n",
        "      return 0.0\n",
        "\n",
        "    def rule_switching(self, rule):\n",
        "        \"\"\"\n",
        "        Serially changing the rules : color - form - number.\n",
        "        \"\"\"\n",
        "        if rule!=2:\n",
        "            rule = rule+1\n",
        "        else:\n",
        "            rule = 0\n",
        "        return rule\n",
        "\n",
        "    def external_feedback(self, action):\n",
        "        \"\"\"\n",
        "        Returns a true reward according to the success or not of the card chosen.\n",
        "        \"\"\"\n",
        "        response_card = self.np_data\n",
        "        reference_cards = self.m_percep\n",
        "        right_action_i = 0\n",
        "\n",
        "        #print(\"Determine reward for rule:\", self.rule, \"and card: \")\n",
        "        #print(response_card)\n",
        "\n",
        "        for i in range(0, self.nb_templates):\n",
        "\n",
        "            if np.array_equal(reference_cards[i][self.rule], response_card[self.rule]):\n",
        "                right_action_i = i\n",
        "\n",
        "        if right_action_i == action:\n",
        "            #0 to decrease error activity\n",
        "            return 0\n",
        "        else:\n",
        "            #1 to activate error cluster\n",
        "            return 1\n",
        "\n",
        "    def step(self, action: int):\n",
        "\n",
        "        if self._reset_next_step:\n",
        "            return self.reset()\n",
        "\n",
        "        agent_action = WCST_Env.ACTIONS[action]\n",
        "\n",
        "        #Compute reward\n",
        "        step_reward = self.external_feedback(agent_action)\n",
        "        print(\"Reward:\", step_reward)\n",
        "\n",
        "        ##Winstreak count\n",
        "        if step_reward == 0:\n",
        "            self.t_err = 0\n",
        "            self.nb_win += 1\n",
        "            self.winstreak += 1\n",
        "            #ptrial.append(1)\n",
        "            #ntrial.append(0)\n",
        "\n",
        "        if step_reward == 1:\n",
        "            self.t_criterion += 1\n",
        "            self.t_err += 1\n",
        "\n",
        "            #FIXME: In the original code winstreak is reset\n",
        "            #after a positive reward. This doesn't look right.\n",
        "\n",
        "            #self.winstreak = 0\n",
        "            #ptrial.append(0)\n",
        "            #ntrial.append(1)\n",
        "\n",
        "        # Criterion test\n",
        "        # After 3 wins, then change the rule\n",
        "        if self.winstreak==3:\n",
        "            self.rule = self.rule_switching(self.rule)\n",
        "            self.criterions.append(self.t_criterion)\n",
        "            #Reset some variables and increment nbTS\n",
        "            self.t_criterion = 0\n",
        "            self.winstreak = 0\n",
        "            self.nbTS +=1\n",
        "            #print(\"winstreak=3, New rule is \", self.rule, \"NbTS=\", self.nbTS)\n",
        "        else:\n",
        "            #print(\"Winstreak=\", self.winstreak)\n",
        "            pass\n",
        "\n",
        "        self._action_history.append(agent_action)\n",
        "        self._current_step += 1\n",
        "\n",
        "        # Check for termination.\n",
        "        if self.nbTS >= 6 or self._current_step == (self.episode_steps - 1):\n",
        "            self._reset_next_step = True\n",
        "            #print(\"B. Return last observation and terminate, NbTS=\", self.nbTS)\n",
        "            return dm_env.termination(reward=self._episode_return(), observation=self._observation())\n",
        "        else:\n",
        "            #Send reward to agent and a new observation\n",
        "            #print(\"A. Step: \", self._current_step, \"Return next observation\")\n",
        "            return dm_env.transition(reward=step_reward, observation=self._observation())\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return dm_env.specs.BoundedArray(\n",
        "            shape=self.sample_card.shape,\n",
        "            dtype=self.sample_card.dtype,\n",
        "            name='card',\n",
        "            minimum=len(self.sample_card),\n",
        "            maximum=len(self.sample_card)\n",
        "        )\n",
        "\n",
        "    def action_spec(self):\n",
        "        return dm_env.specs.DiscreteArray(\n",
        "            num_values=len(WCST_Env.ACTIONS),\n",
        "            dtype=np.int32,\n",
        "            name='action')\n",
        "        pass\n",
        "\n",
        "    def _observation(self):\n",
        "        # agent observes only the current trial\n",
        "\n",
        "        #INPUT new card, (Environment)\n",
        "        print(\"Calling new_card...\")\n",
        "        np_data, card = self.new_card()\n",
        "\n",
        "        #print(\"New card is np_data\", np_data)\n",
        "        print(\"New card is v_data\", card)\n",
        "\n",
        "        obs = card\n",
        "        return obs\n",
        "\n",
        "    @staticmethod\n",
        "    def create_environment():\n",
        "        \"\"\"Utility function to create a N-back environment and its spec.\"\"\"\n",
        "\n",
        "        # Make sure the environment outputs single-precision floats.\n",
        "        environment = wrappers.SinglePrecisionWrapper(WCST_Env())\n",
        "\n",
        "        # Grab the spec of the environment.\n",
        "        environment_spec = specs.make_environment_spec(environment)\n",
        "        return environment, environment_spec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "MGhCqTtE2nO5"
      },
      "source": [
        "### Initialize the environment and the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "execution": {},
        "id": "_XSWBTOd2nO5"
      },
      "outputs": [],
      "source": [
        "env, env_spec = WCST_Env.create_environment()\n",
        "agent = RandomAgent(env_spec)\n",
        "#print('actions:\\n', env_spec.actions)\n",
        "#print('observations:\\n', env_spec.observations)\n",
        "#print('rewards:\\n', env_spec.rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "mTiFurZ_2nO6"
      },
      "source": [
        "### Run the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "execution": {},
        "id": "xjCQ0AhA2nO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03d1b76-3ebe-4ab3-d576-1452fe7e168c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "{'episode': 0, 'episode_length': 35, 'episode_return': 25.0}\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
            "Reward: 1\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
            "Reward: 0\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "{'episode': 1, 'episode_length': 29, 'episode_return': 20.0}\n",
            "DONE\n"
          ]
        }
      ],
      "source": [
        "# fitting parameters\n",
        "n_episodes = 2\n",
        "n_total_steps = 0\n",
        "log_loss = False\n",
        "n_steps = n_episodes * 36\n",
        "all_returns = []\n",
        "\n",
        "# main loop\n",
        "for episode in range(n_episodes):\n",
        "  episode_steps = 0\n",
        "  episode_return = 0\n",
        "  episode_loss = 0\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  timestep = env.reset()\n",
        "\n",
        "  # Make the first observation.\n",
        "  agent.observe_first(timestep)\n",
        "\n",
        "  # Run an episode\n",
        "  while not timestep.last():\n",
        "\n",
        "    # DEBUG\n",
        "    # print(timestep)\n",
        "\n",
        "    # Generate an action from the agent's policy and step the environment.\n",
        "    action = agent.select_action(timestep.observation)\n",
        "    timestep = env.step(action)\n",
        "\n",
        "    # Have the agent observe the timestep and let the agent update itself.\n",
        "    agent.observe(action, next_timestep=timestep)\n",
        "    agent.update()\n",
        "\n",
        "    # Book-keeping.\n",
        "    episode_steps += 1\n",
        "    n_total_steps += 1\n",
        "    episode_return += timestep.reward\n",
        "\n",
        "    if log_loss:\n",
        "      episode_loss += agent.last_loss\n",
        "\n",
        "    if n_steps is not None and n_total_steps >= n_steps:\n",
        "      break\n",
        "\n",
        "  # Collect the results and combine with counts.\n",
        "  steps_per_second = episode_steps / (time.time() - start_time)\n",
        "  result = {\n",
        "      'episode': episode,\n",
        "      'episode_length': episode_steps,\n",
        "      'episode_return': episode_return,\n",
        "  }\n",
        "  if log_loss:\n",
        "    result['loss_avg'] = episode_loss/episode_steps\n",
        "\n",
        "  all_returns.append(episode_return)\n",
        "\n",
        "  #display(env.plot_state())\n",
        "  # Log the given results.\n",
        "  print(result)\n",
        "\n",
        "  if n_steps is not None and n_total_steps >= n_steps:\n",
        "    break\n",
        "\n",
        "print(\"DONE\")\n",
        "\n",
        "#clear_output()\n",
        "\n",
        "# Histogram of all returns\n",
        "#plt.figure()\n",
        "#sns.histplot(all_returns, stat=\"density\", kde=True, bins=12)\n",
        "#plt.xlabel('Return [a.u.]')\n",
        "#plt.ylabel('Density')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xgUDEuym2nO7"
      },
      "source": [
        "**Note:** You can simplify the environment loop using [DeepMind Acme](https://github.com/deepmind/acme)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "l6wSUwW82nO8"
      },
      "outputs": [],
      "source": [
        "# init a new N-back environment\n",
        "env, env_spec = NBack.create_environment()\n",
        "\n",
        "# DEBUG fake testing environment.\n",
        "# Uncomment this to debug your agent without using the N-back environment.\n",
        "# env = fakes.DiscreteEnvironment(\n",
        "#     num_actions=2,\n",
        "#     num_observations=1000,\n",
        "#     obs_dtype=np.float32,\n",
        "#     episode_length=32)\n",
        "# env_spec = specs.make_environment_spec(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "nFpI1uaN2nO9"
      },
      "outputs": [],
      "source": [
        "def dqn_make_network(action_spec: specs.DiscreteArray) -> snt.Module:\n",
        "  return snt.Sequential([\n",
        "      snt.Flatten(),\n",
        "      snt.nets.MLP([50, 50, action_spec.num_values]),\n",
        "  ])\n",
        "\n",
        "# construct a DQN agent\n",
        "agent = dqn.DQN(\n",
        "    environment_spec=env_spec,\n",
        "    network=dqn_make_network(env_spec.actions),\n",
        "    epsilon=[0.5],\n",
        "    logger=loggers.InMemoryLogger(),\n",
        "    checkpoint=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JfcFCx1L2nO9"
      },
      "source": [
        "Now, we run the environment loop with the DQN agent and print the training log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "ZoG8b0FF2nO-"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "loop = EnvironmentLoop(env, agent, logger=loggers.InMemoryLogger())\n",
        "#loop.run(n_episodes)\n",
        "\n",
        "# print logs\n",
        "#logs = pd.DataFrame(loop._logger._data)\n",
        "#logs.tail()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}