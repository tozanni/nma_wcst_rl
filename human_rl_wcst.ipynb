{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tozanni/nma_wcst_rl/blob/main/human_rl_wcst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BHbUO_QW2nOV"
      },
      "source": [
        "# Using RL to Model Wisconsin Card Sorting Task\n",
        "\n",
        "\n",
        "**Original notebook credits:**\n",
        "\n",
        "__Content creators:__ Morteza Ansarinia, Yamil Vidal\n",
        "\n",
        "__Production editor:__ Spiros Chavlis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kBCTQKcY2nOg"
      },
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "- This project aims to use behavioral data to train an agent and then use the agent to investigate data produced by human subjects. Having a computational agent that mimics humans in such tests, we will be able to compare its mechanics with human data.\n",
        "\n",
        "- In another conception, we could fit an agent that learns many cognitive tasks that require abstract-level constructs such as executive functions. This is a multi-task control problem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ApJnFBK72nOi"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "yHw6zo6R2nOj",
        "outputId": "c137d296-946d-42a8-87ab-5ef07cd4b31c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.5/318.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for dm-acme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.4/268.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.82 requires numpy>=1.25.0, but you have numpy 1.23.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!pip install jedi --quiet\n",
        "!pip install --upgrade pip setuptools wheel --quiet\n",
        "!pip install dm-acme[jax] --quiet\n",
        "!pip install dm-sonnet --quiet\n",
        "!pip install trfl --quiet\n",
        "!pip install numpy==1.23.3 --quiet --ignore-installed\n",
        "!pip uninstall seaborn -y --quiet\n",
        "!pip install seaborn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {},
        "id": "8q8RNDSn2nOq"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sonnet as snt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import dm_env\n",
        "\n",
        "import acme\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme import EnvironmentLoop\n",
        "from acme.agents.tf import dqn\n",
        "from acme.utils import loggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "5j0ZUwlV2nOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d47bbfc-69d7-41b8-a9b6-fb8030dc9fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# @title Figure settings\n",
        "from IPython.display import clear_output, display, HTML\n",
        "%matplotlib inline\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "yeSwZpB_2nOt"
      },
      "source": [
        "---\n",
        "# Background\n",
        "\n",
        "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Jh-sqwiF2nOw"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "This notebook works on simulated data only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "liKW-N012nOx"
      },
      "source": [
        "## Wisconsin Card Sorting task (WCST)\n",
        "\n",
        "TODO: Describe Task\n",
        "\n",
        "TODO: Describe metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EdljINN32nOy"
      },
      "source": [
        "---\n",
        "# Cognitive Tests Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "vvv7SFOc2nO1"
      },
      "source": [
        "## Implementation scheme\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import WCST"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UCK5IPrgRaY",
        "outputId": "c2df7e1c-f0a2-4c49-8ec3-c98f18428051"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "module"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "AgKPh5QN2nO2"
      },
      "source": [
        "### Environment\n",
        "\n",
        "The following cell implments an envinronment for the WCST:\n",
        "- Rewards the agent once the action was correct (i.e., a normative model of the environment).\n",
        "- **Future work**: Receives human data and returns what participants performed as the observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "execution": {},
        "id": "__V0gMzt2nO2"
      },
      "outputs": [],
      "source": [
        "class WCST_Env(dm_env.Environment):\n",
        "    ACTIONS = [0, 1, 2, 3]\n",
        "\n",
        "    def __init__(self,seed=1):\n",
        "\n",
        "        self.episode_steps = 36  #36 cards or steps per episode\n",
        "        self._current_step = 0  #Current episode step counter\n",
        "        self._reset_next_step = True\n",
        "        self._action_history = []\n",
        "\n",
        "        #Init WCST variables\n",
        "        self.nb_dim = 3\n",
        "        self.nb_features = 4\n",
        "        self.nb_templates = self.nb_features\n",
        "        r = 3  #rules number, we have 3 rules\n",
        "\n",
        "        self.sample_card = np.array([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1])\n",
        "\n",
        "        self.nbTS = 0\n",
        "        self.nb_win = 0\n",
        "        self.t_criterion = 0\n",
        "        self.t_err = 0\n",
        "        self.criterions = []\n",
        "\n",
        "        self.winstreak = 0\n",
        "        self.m_percep = WCST.perception(self.nb_dim, self.nb_templates, self.nb_features)\n",
        "        self.reasoning_list = []\n",
        "        self.rule = 0\n",
        "\n",
        "        #Last card info, it's dealt for the first time on reset method\n",
        "        self.np_data = []\n",
        "        self.v_data = []\n",
        "\n",
        "    def new_card(self):\n",
        "        v_data = [] #list type\n",
        "        np_data = WCST.response_item_Reasoning(self.nb_dim, self.nb_features, self.m_percep, self.reasoning_list) #Modified WCST version\n",
        "\n",
        "        #Transform into a vector\n",
        "        for arr in np_data:\n",
        "            for e in arr:\n",
        "                v_data.append(e)\n",
        "\n",
        "        #Save last card info\n",
        "        self.np_data = np_data\n",
        "        self.v_data = v_data\n",
        "\n",
        "        return np_data, v_data\n",
        "\n",
        "    def reset(self):\n",
        "        self._reset_next_step = False\n",
        "        self._current_step = 0\n",
        "        self._action_history.clear()\n",
        "\n",
        "        #Deal new card\n",
        "        obs = self._observation()\n",
        "        #self._current_step += 1\n",
        "        return dm_env.restart(self._observation())\n",
        "\n",
        "    def _episode_return(self):\n",
        "      return 0.0\n",
        "\n",
        "    def rule_switching(self, rule):\n",
        "        \"\"\"\n",
        "        Serially changing the rules : color - form - number.\n",
        "        \"\"\"\n",
        "        if rule!=2:\n",
        "            rule = rule+1\n",
        "        else:\n",
        "            rule = 0\n",
        "        return rule\n",
        "\n",
        "    def external_feedback(self, action):\n",
        "        \"\"\"\n",
        "        Returns a true reward according to the success or not of the card chosen.\n",
        "        \"\"\"\n",
        "        response_card = self.np_data\n",
        "        reference_cards = self.m_percep\n",
        "        right_action_i = 0\n",
        "\n",
        "        #print(\"Determine reward for rule:\", self.rule, \"and card: \")\n",
        "        #print(response_card)\n",
        "\n",
        "        for i in range(0, self.nb_templates):\n",
        "\n",
        "            if np.array_equal(reference_cards[i][self.rule], response_card[self.rule]):\n",
        "                right_action_i = i\n",
        "\n",
        "        if right_action_i == action:\n",
        "            #0 to decrease error activity\n",
        "            return 0\n",
        "        else:\n",
        "            #1 to activate error cluster\n",
        "            return 1\n",
        "\n",
        "\n",
        "    def step(self, action: int):\n",
        "\n",
        "        if self._reset_next_step:\n",
        "            return self.reset()\n",
        "\n",
        "        agent_action = WCST_Env.ACTIONS[action]\n",
        "\n",
        "        #Compute reward\n",
        "        step_reward = self.external_feedback(agent_action)\n",
        "        print(\"Reward:\", step_reward)\n",
        "\n",
        "        ##Winstreak count\n",
        "        if step_reward == 0:\n",
        "            self.t_err = 0\n",
        "            self.nb_win += 1\n",
        "            self.winstreak += 1\n",
        "            #ptrial.append(1)\n",
        "            #ntrial.append(0)\n",
        "\n",
        "        if step_reward == 1:\n",
        "            self.t_criterion += 1\n",
        "            self.t_err += 1\n",
        "\n",
        "            #FIXME: In the original code winstreak is reset\n",
        "            #after a positive reward. This doesn't look right.\n",
        "\n",
        "            #self.winstreak = 0\n",
        "            #ptrial.append(0)\n",
        "            #ntrial.append(1)\n",
        "\n",
        "        # Criterion test\n",
        "        # After 3 wins, then change the rule\n",
        "        if self.winstreak==3:\n",
        "            self.rule = self.rule_switching(self.rule)\n",
        "            self.criterions.append(self.t_criterion)\n",
        "            #Reset some variables and increment nbTS\n",
        "            self.t_criterion = 0\n",
        "            self.winstreak = 0\n",
        "            self.nbTS +=1\n",
        "            print(\"winstreak=3, New rule is \", self.rule, \"NbTS=\", self.nbTS)\n",
        "        else:\n",
        "            #print(\"Winstreak=\", self.winstreak)\n",
        "            pass\n",
        "\n",
        "        self._action_history.append(agent_action)\n",
        "        self._current_step += 1\n",
        "\n",
        "        # Check for termination.\n",
        "        if self.nbTS >= 6 or self._current_step == self.episode_steps:\n",
        "            self._reset_next_step = True\n",
        "            print(\"A. Return last observation and terminate, NbTS=\", self.nbTS)\n",
        "            return dm_env.termination(reward=self._episode_return(), observation=self._observation())\n",
        "        else:\n",
        "            #Send reward to agent and a new observation\n",
        "            #Uncomment in notebook\n",
        "            print(\"B. Step: \", self._current_step, \"Return observation\")\n",
        "            return dm_env.transition(reward=step_reward, observation=self._observation())\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return dm_env.specs.BoundedArray(\n",
        "            shape=self.sample_card.shape,\n",
        "            dtype=self.sample_card.dtype,\n",
        "            name='card',\n",
        "            minimum=len(self.sample_card),\n",
        "            maximum=len(self.sample_card)\n",
        "        )\n",
        "\n",
        "    def action_spec(self):\n",
        "        return dm_env.specs.DiscreteArray(\n",
        "            num_values=len(WCST_Env.ACTIONS),\n",
        "            dtype=np.int32,\n",
        "            name='action')\n",
        "        pass\n",
        "\n",
        "    def _observation(self):\n",
        "        # agent observes only the current trial\n",
        "\n",
        "        #INPUT new card, (Environment)\n",
        "        print(\"Calling new_card...\")\n",
        "        np_data, card = self.new_card()\n",
        "\n",
        "        #print(\"New card is np_data\", np_data)\n",
        "        print(\"New card is v_data\", card)\n",
        "\n",
        "        obs = card\n",
        "        return obs\n",
        "\n",
        "    @staticmethod\n",
        "    def create_environment():\n",
        "        \"\"\"Utility function to create a N-back environment and its spec.\"\"\"\n",
        "\n",
        "        # Make sure the environment outputs single-precision floats.\n",
        "        environment = wrappers.SinglePrecisionWrapper(WCST_Env())\n",
        "\n",
        "        # Grab the spec of the environment.\n",
        "        environment_spec = specs.make_environment_spec(environment)\n",
        "        return environment, environment_spec\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env, env_spec = WCST_Env.create_environment()\n",
        "agent = RandomAgent(env_spec)\n",
        "\n",
        "#print('actions:\\n', env_spec.actions)\n",
        "#print('observations:\\n', env_spec.observations)\n",
        "#print('rewards:\\n', env_spec.rewards)"
      ],
      "metadata": {
        "id": "0G-arXyJoIyu"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test run\n",
        "env = WCST_Env()\n",
        "\n",
        "# First observation\n",
        "timestep = env.reset()\n",
        "\n",
        "import random\n",
        "\n",
        "## FIXME: This stalls after 36 cards, which is the limit per game or episode\n",
        "## In the RL framework you should iterate as it is indicated in the acme loop.\n",
        "for i in range(0,37):\n",
        "\n",
        "    print(\"Step \", i)\n",
        "    action = random.randint(0,2)\n",
        "    env.step(action)\n",
        "\n",
        "print(\"END\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aXeG7B-Vcb20",
        "outputId": "b8dce1e8-715d-4805-d2a9-7d1f497afb72"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "Step  0\n",
            "Reward: 1\n",
            "B. Step:  1 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
            "Step  1\n",
            "Reward: 1\n",
            "B. Step:  2 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
            "Step  2\n",
            "Reward: 0\n",
            "B. Step:  3 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n",
            "Step  3\n",
            "Reward: 1\n",
            "B. Step:  4 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
            "Step  4\n",
            "Reward: 1\n",
            "B. Step:  5 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "Step  5\n",
            "Reward: 1\n",
            "B. Step:  6 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "Step  6\n",
            "Reward: 1\n",
            "B. Step:  7 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
            "Step  7\n",
            "Reward: 1\n",
            "B. Step:  8 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "Step  8\n",
            "Reward: 1\n",
            "B. Step:  9 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "Step  9\n",
            "Reward: 0\n",
            "B. Step:  10 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "Step  10\n",
            "Reward: 1\n",
            "B. Step:  11 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "Step  11\n",
            "Reward: 1\n",
            "B. Step:  12 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
            "Step  12\n",
            "Reward: 0\n",
            "winstreak=3, New rule is  1 NbTS= 1\n",
            "B. Step:  13 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "Step  13\n",
            "Reward: 1\n",
            "B. Step:  14 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
            "Step  14\n",
            "Reward: 1\n",
            "B. Step:  15 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "Step  15\n",
            "Reward: 1\n",
            "B. Step:  16 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "Step  16\n",
            "Reward: 1\n",
            "B. Step:  17 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "Step  17\n",
            "Reward: 1\n",
            "B. Step:  18 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
            "Step  18\n",
            "Reward: 0\n",
            "B. Step:  19 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "Step  19\n",
            "Reward: 0\n",
            "B. Step:  20 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
            "Step  20\n",
            "Reward: 1\n",
            "B. Step:  21 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "Step  21\n",
            "Reward: 1\n",
            "B. Step:  22 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
            "Step  22\n",
            "Reward: 1\n",
            "B. Step:  23 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "Step  23\n",
            "Reward: 1\n",
            "B. Step:  24 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
            "Step  24\n",
            "Reward: 1\n",
            "B. Step:  25 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "Step  25\n",
            "Reward: 1\n",
            "B. Step:  26 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
            "Step  26\n",
            "Reward: 1\n",
            "B. Step:  27 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
            "Step  27\n",
            "Reward: 0\n",
            "winstreak=3, New rule is  2 NbTS= 2\n",
            "B. Step:  28 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "Step  28\n",
            "Reward: 1\n",
            "B. Step:  29 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "Step  29\n",
            "Reward: 1\n",
            "B. Step:  30 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]\n",
            "Step  30\n",
            "Reward: 1\n",
            "B. Step:  31 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
            "Step  31\n",
            "Reward: 0\n",
            "B. Step:  32 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "Step  32\n",
            "Reward: 1\n",
            "B. Step:  33 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
            "Step  33\n",
            "Reward: 1\n",
            "B. Step:  34 Return observation\n",
            "Calling new_card...\n",
            "New card is v_data [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "Step  34\n",
            "Reward: 1\n",
            "B. Step:  35 Return observation\n",
            "Calling new_card...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-aa2e494057b8>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"END\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-6cb56e04d917>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m#Uncomment in notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B. Step: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Return observation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdm_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-6cb56e04d917>\u001b[0m in \u001b[0;36m_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m#INPUT new card, (Environment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calling new_card...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mnp_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m#print(\"New card is np_data\", np_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-6cb56e04d917>\u001b[0m in \u001b[0;36mnew_card\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mv_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#list type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mnp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWCST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_item_Reasoning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_percep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreasoning_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Modified WCST version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#Transform into a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WCST.py\u001b[0m in \u001b[0;36mresponse_item_Reasoning\u001b[0;34m(nb_dim, nb_features, m_percep, reasoning_list)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_card_Reasoning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_equality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_percep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0munique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_unity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreasoning_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WCST.py\u001b[0m in \u001b[0;36mcheck_equality\u001b[0;34m(m_percep, item, nb_dim)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mdim_eq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_percep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mdim_eq\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36marray_equal\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36marray_equal\u001b[0;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[1;32m   2447\u001b[0m     \u001b[0mconsidered\u001b[0m \u001b[0mequal\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meither\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreal\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;32mor\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimaginary\u001b[0m \u001b[0mcomponents\u001b[0m \u001b[0mare\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2449\u001b[0;31m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2450\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rDXuNYEN2nO4"
      },
      "source": [
        "### Define a random agent\n",
        "\n",
        "For more information you can refer to NMA-DL W3D2 Basic Reinforcement learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {},
        "id": "PTGxhONI2nO4"
      },
      "outputs": [],
      "source": [
        "class RandomAgent(acme.Actor):\n",
        "\n",
        "  def __init__(self, environment_spec):\n",
        "    \"\"\"Gets the number of available actions from the environment spec.\"\"\"\n",
        "    self._num_actions = environment_spec.actions.num_values\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    \"\"\"Selects an action uniformly at random.\"\"\"\n",
        "    action = np.random.randint(self._num_actions)\n",
        "    return action\n",
        "\n",
        "  def observe_first(self, timestep):\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def update(self):\n",
        "    \"\"\"Does not update as the RandomAgent does not learn from data.\"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "MGhCqTtE2nO5"
      },
      "source": [
        "### Initialize the environment and the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "execution": {},
        "id": "_XSWBTOd2nO5",
        "outputId": "20a2d873-734b-4b8c-f592-9f46b5fa1b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-c0ca0e568231>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWCST_Env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print('actions:\\n', env_spec.actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print('observations:\\n', env_spec.observations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-5153f3eb28a6>\u001b[0m in \u001b[0;36mcreate_environment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Grab the spec of the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0menvironment_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_environment_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;31m#return environment, environment_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/acme/specs.py\u001b[0m in \u001b[0;36mmake_environment_spec\u001b[0;34m(environment)\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;34m\"\"\"Returns an `EnvironmentSpec` describing values used by an environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   return EnvironmentSpec(\n\u001b[0;32m---> 46\u001b[0;31m       \u001b[0mobservations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m       \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/acme/wrappers/single_precision.py\u001b[0m in \u001b[0;36mobservation_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreward_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-5153f3eb28a6>\u001b[0m in \u001b[0;36mobservation_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         return dm_env.specs.BoundedArray(\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcard_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcard_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'card'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'WCST_Env' object has no attribute 'card_sample'"
          ]
        }
      ],
      "source": [
        "env, env_spec = WCST_Env.create_environment()\n",
        "agent = RandomAgent(env_spec)\n",
        "\n",
        "#print('actions:\\n', env_spec.actions)\n",
        "#print('observations:\\n', env_spec.observations)\n",
        "#print('rewards:\\n', env_spec.rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "mTiFurZ_2nO6"
      },
      "source": [
        "### Run the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "execution": {},
        "id": "xjCQ0AhA2nO6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "62c35908-ca5d-4af9-c927-145ee9a75f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling new_card...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-e45c9f9d0257>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mtimestep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# Make the first observation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/acme/wrappers/single_precision.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdm_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeStep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_timestep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-275f31f2fa42>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#Deal new card\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m#self._current_step += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdm_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-275f31f2fa42>\u001b[0m in \u001b[0;36m_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m#INPUT new card, (Environment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calling new_card...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mnp_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m#print(\"New card is np_data\", np_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-275f31f2fa42>\u001b[0m in \u001b[0;36mnew_card\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mv_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#list type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mnp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWCST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_item_Reasoning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_percep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreasoning_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Modified WCST version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#Transform into a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WCST.py\u001b[0m in \u001b[0;36mresponse_item_Reasoning\u001b[0;34m(nb_dim, nb_features, m_percep, reasoning_list)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_card_Reasoning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_equality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_percep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0munique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_unity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreasoning_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mreasoning_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WCST.py\u001b[0m in \u001b[0;36mcheck_unity\u001b[0;34m(item, cardlist)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_unity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcardlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcardlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WCST.py\u001b[0m in \u001b[0;36minList\u001b[0;34m(array, arraylist)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marraylist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marraylist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36marray_equal\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36marray_equal\u001b[0;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[1;32m   2447\u001b[0m     \u001b[0mconsidered\u001b[0m \u001b[0mequal\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meither\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreal\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;32mor\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimaginary\u001b[0m \u001b[0mcomponents\u001b[0m \u001b[0mare\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2449\u001b[0;31m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2450\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# fitting parameters\n",
        "n_episodes = 100\n",
        "n_total_steps = 0\n",
        "log_loss = False\n",
        "n_steps = n_episodes * 32\n",
        "all_returns = []\n",
        "\n",
        "# main loop\n",
        "for episode in range(n_episodes):\n",
        "  episode_steps = 0\n",
        "  episode_return = 0\n",
        "  episode_loss = 0\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  timestep = env.reset()\n",
        "\n",
        "  # Make the first observation.\n",
        "  agent.observe_first(timestep)\n",
        "\n",
        "  # Run an episode\n",
        "  while not timestep.last():\n",
        "\n",
        "    # DEBUG\n",
        "    # print(timestep)\n",
        "\n",
        "    # Generate an action from the agent's policy and step the environment.\n",
        "    action = agent.select_action(timestep.observation)\n",
        "    timestep = env.step(action)\n",
        "\n",
        "    # Have the agent observe the timestep and let the agent update itself.\n",
        "    agent.observe(action, next_timestep=timestep)\n",
        "    agent.update()\n",
        "\n",
        "    # Book-keeping.\n",
        "    episode_steps += 1\n",
        "    n_total_steps += 1\n",
        "    episode_return += timestep.reward\n",
        "\n",
        "    if log_loss:\n",
        "      episode_loss += agent.last_loss\n",
        "\n",
        "    if n_steps is not None and n_total_steps >= n_steps:\n",
        "      break\n",
        "\n",
        "  # Collect the results and combine with counts.\n",
        "  steps_per_second = episode_steps / (time.time() - start_time)\n",
        "  result = {\n",
        "      'episode': episode,\n",
        "      'episode_length': episode_steps,\n",
        "      'episode_return': episode_return,\n",
        "  }\n",
        "  if log_loss:\n",
        "    result['loss_avg'] = episode_loss/episode_steps\n",
        "\n",
        "  all_returns.append(episode_return)\n",
        "\n",
        "  display(env.plot_state())\n",
        "  # Log the given results.\n",
        "  print(result)\n",
        "\n",
        "  if n_steps is not None and n_total_steps >= n_steps:\n",
        "    break\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Histogram of all returns\n",
        "#plt.figure()\n",
        "#sns.histplot(all_returns, stat=\"density\", kde=True, bins=12)\n",
        "#plt.xlabel('Return [a.u.]')\n",
        "#plt.ylabel('Density')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xgUDEuym2nO7"
      },
      "source": [
        "**Note:** You can simplify the environment loop using [DeepMind Acme](https://github.com/deepmind/acme)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "l6wSUwW82nO8"
      },
      "outputs": [],
      "source": [
        "# init a new N-back environment\n",
        "env, env_spec = NBack.create_environment()\n",
        "\n",
        "# DEBUG fake testing environment.\n",
        "# Uncomment this to debug your agent without using the N-back environment.\n",
        "# env = fakes.DiscreteEnvironment(\n",
        "#     num_actions=2,\n",
        "#     num_observations=1000,\n",
        "#     obs_dtype=np.float32,\n",
        "#     episode_length=32)\n",
        "# env_spec = specs.make_environment_spec(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "nFpI1uaN2nO9"
      },
      "outputs": [],
      "source": [
        "def dqn_make_network(action_spec: specs.DiscreteArray) -> snt.Module:\n",
        "  return snt.Sequential([\n",
        "      snt.Flatten(),\n",
        "      snt.nets.MLP([50, 50, action_spec.num_values]),\n",
        "  ])\n",
        "\n",
        "# construct a DQN agent\n",
        "agent = dqn.DQN(\n",
        "    environment_spec=env_spec,\n",
        "    network=dqn_make_network(env_spec.actions),\n",
        "    epsilon=[0.5],\n",
        "    logger=loggers.InMemoryLogger(),\n",
        "    checkpoint=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JfcFCx1L2nO9"
      },
      "source": [
        "Now, we run the environment loop with the DQN agent and print the training log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "ZoG8b0FF2nO-"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "loop = EnvironmentLoop(env, agent, logger=loggers.InMemoryLogger())\n",
        "#loop.run(n_episodes)\n",
        "\n",
        "# print logs\n",
        "#logs = pd.DataFrame(loop._logger._data)\n",
        "#logs.tail()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}